---
title: SVM相关问题
date: 2018-01-30 15:40:25
tags: 机器学习
categories: 机器学习
---

1. 支持向量机是一种线性分类器 ：基本思想是在两类线性可分条件下，所设计的分类器界面使两类之间的间隔为最大，它的基本出发点是使期望泛化风险尽可能小。（使用核函数可解决非线性问题）

2. SVM的目标是找到使得训练数据尽可能分开且分类间隔最大的超平面，应该属于结构风险最小化。
3. SVM可以通过正则化系数控制模型的复杂度，避免过拟合。

![](/images/SVM相关问题.png)

4. A正确。考虑加入正则化项的原因：想象一个完美的数据集，`y>1`是正类，`y<-1`是负类，决策面`y=0`，加入一个`y=-30`的正类噪声样本，那么决策面将会变“歪”很多，分类间隔变小，泛化能力减小。加入正则项之后，对噪声样本的容错能力增强，前面提到的例子里面，决策面就会没那么“歪”了，使得分类间隔变大，提高了泛化能力。

B正确，Hinge损失是0-1损失函数的一种代理函数，Hinge损失的具体形式如下：
````
max(0,1−m)
````
运用Hinge损失的典型分类器是SVM算法。

C错误。间隔应该是`2/||w||`才对，后半句应该没错，向量的模通常指的就是其二范数。

D正确。考虑软间隔的时候，C对优化问题的影响就在于把a的范围从[0，+inf]限制到了[0,C]。C越小，那么a就会越小，目标函数拉格朗日函数导数为0可以求出w=求和ai∗yi∗xi，a变小使得w变小，因此间隔2/||w||变大
来源：@刘炫320，链接：[http://blog.csdn.net/column/details/16442.html](http://blog.csdn.net/column/details/16442.html)

[http://blog.csdn.net/timcompp/article/details/62237986](http://blog.csdn.net/timcompp/article/details/62237986)
# svm算法的原理、如何组织训练数据、如何调节惩罚因子、如何防止过拟合、svm的泛化能力、增量学习 

1. SVM是一种二类分类的模型，它的基本模型是在特征空间中寻找间隔最大化的分离超平面的线性分类器。 
2.  
3. 惩罚因子C决定了你有多重视离群点带来的损失，显然当所有离群点的松弛变量的和一定时，你定的C越大，对目标函数的损失也越大，此时就暗示着你非常不愿意放弃这些离群点，最极端的情况是你把C定为无限大，这样只要稍有一个点离群，目标函数的值马上变成无限大，马上让问题变成无解，这就退化成了硬间隔问题。 
惩罚因子C不是一个变量，整个优化问题在解的时候，C是一个你必须事先指定的值，指定这个值以后，解一下，得到一个分类器，然后用测试数据看看结果怎么样，如果不够好，换一个C的值，再解一次优化问题，得到另一个分类器，再看看效果，如此就是一个参数寻优的过程，但这和优化问题本身决不是一回事，优化问题在解的过程中，C一直是定值，要记住。
4. 解决过拟合的办法是为SVM引入了松弛变量ξ（slack variable），将SVM公式的约束条件改为。因为松弛变量能够容忍异常点的存在，我们的支持向量和超平面都不会受到它的影响。 
我们加上松弛变量的平方和，并求最小值。这样就达到一个平衡：既希望松弛变量存在以解决异常点问题，又不希望松弛变量太大导致分类解决太差。 


# LR和SVM的联系与区别 

## 联系 
1. LR和SVM都可以处理分类问题，且一般都用于处理线性二分类问题（在改进的情况下可以处理多分类问题） 
2. 两个方法都可以增加不同的正则化项，如l1、l2等等。所以在很多实验中，两种算法的结果是很接近的。

## 区别

1. LR是参数模型，SVM是非参数模型。 
2. 从目标函数来看，区别在于逻辑回归采用的是logistical loss，SVM采用的是hinge loss.这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。 
3. SVM的处理方法是只考虑support vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。 
4. 逻辑回归相对来说模型更简单，好理解，特别是大规模线性分类时比较方便。而SVM的理解和优化相对来说复杂一些，SVM转化为对偶问题后,分类只需要计算与少数几个支持向量的距离,这个在进行复杂核函数计算时优势很明显,能够大大简化模型和计算。 
5. logic 能做的 svm能做，但可能在准确率上有问题，svm能做的logic有的做不了。
